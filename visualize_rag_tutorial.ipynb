{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize  your RAG Data - EDA for Retrieval-Augmented Generation\n",
    "## How to use UMAP dimensionality reduction for Embeddings to show  Questions, Answers and their relationships to source documents with OpenAI, Langchain and ChromaDB\n",
    "This notebook is part of an [article at ITNEXT.](https://itnext.io/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai chromadb renumics-spotlight\n",
    "%env OPENAI_API_KEY=<your-api-key>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings model and vector store\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "docs_vectorstore = Chroma(\n",
    "    collection_name=\"docs_store\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"docs-db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents with the LangChain document loader\n",
    "from langchain_community.document_loaders import BSHTMLLoader, DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"docs\",\n",
    "    glob=\"*.html\",\n",
    "    loader_cls=BSHTMLLoader,\n",
    "    loader_kwargs={\"open_encoding\": \"utf-8\"},\n",
    "    recursive=True,\n",
    "    show_progress=True,\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide documents into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add documents to the vector store - use an id that can be reconstructed from the metadata\n",
    "import hashlib\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def stable_hash(doc: Document) -> str:\n",
    "    \"\"\"\n",
    "    Stable hash document based on its metadata.\n",
    "    \"\"\"\n",
    "    return hashlib.sha1(json.dumps(doc.metadata, sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "\n",
    "split_ids = list(map(stable_hash, splits))\n",
    "docs_vectorstore.add_documents(splits, ids=split_ids)\n",
    "docs_vectorstore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create language model and retriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "retriever = docs_vectorstore.as_retriever(search_kwargs={\"k\": 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a RAG prompt that includes the question and the source documents\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{source_documents}\n",
    "=========\n",
    "FINAL ANSWER: \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a RAG chain that retrieves documents, generates an answer, and formats the answer\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Content: {doc.page_content}\\nSource: {doc.metadata['source']}\" for doc in docs\n",
    "    )\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(\n",
    "        source_documents=(lambda x: format_docs(x[\"source_documents\"]))\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain = RunnableParallel(\n",
    "    {\n",
    "        \"source_documents\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who built the nuerburgring\"\n",
    "response = rag_chain.invoke(question)\n",
    "answer = response[\"answer\"]\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract embeddings for the documents from the vector store and store them in a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "response = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": response[\"ids\"],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in response[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in response[\"metadatas\"]],\n",
    "        \"document\": response[\"documents\"],\n",
    "        \"embedding\": response[\"embeddings\"],\n",
    "    }\n",
    ")\n",
    "df[\"contains_answer\"] = df[\"document\"].apply(lambda x: \"Eichler\" in x)\n",
    "df[\"contains_answer\"].to_numpy().nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the question and answer with their embeddings to the dataframe\n",
    "question_row = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [\"question\"],\n",
    "        \"question\": [question],\n",
    "        \"embedding\": [embeddings_model.embed_query(question)],\n",
    "    }\n",
    ")\n",
    "answer_row = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [\"answer\"],\n",
    "        \"answer\": [answer],\n",
    "        \"embedding\": [embeddings_model.embed_query(answer)],\n",
    "    }\n",
    ")\n",
    "df = pd.concat([question_row, answer_row, df])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance (L2 norm) between the question and the document embeddings\n",
    "import numpy as np\n",
    "\n",
    "question_embedding = embeddings_model.embed_query(question)\n",
    "df[\"dist\"] = df.apply(\n",
    "    lambda row: np.linalg.norm(np.array(row[\"embedding\"]) - question_embedding),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the dataframe with the question and answer in spotlight\n",
    "from renumics import spotlight\n",
    "\n",
    "spotlight.show(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-demo-IMu3vKF7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
